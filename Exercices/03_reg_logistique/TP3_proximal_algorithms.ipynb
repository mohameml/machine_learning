{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Fig/Ensimag.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Ensimag 2A</h3></center>\n",
    "<hr>\n",
    "<center><h1>Optimisation Num√©rique</h1></center>\n",
    "<center><h2>TP3: Proximal Algorithms (2x1.5h)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of an optimization program\n",
    "\n",
    "An optimization program can be practically divided into three parts:\n",
    "* the *run* environment, in which you test, run your program, and display results.\n",
    "* the *problem* part, which contains the function oracles, problem constraints, etc.\n",
    "* the *algorithmic* part, where the algorithms are coded.\n",
    "\n",
    "The main interest of such division is that these parts are interchangeable, meaning that, for instance, the algorithms of the third part can be used of a variety of problems. That is why such a decomposition is widely used.\n",
    "\n",
    "In the present lab, you will use this division:\n",
    "* `TP3_Proximal_algorithms.ipynb` will be the *run* environment\n",
    "* `logistic_regression.ipynb` will be the considered *logistic regression problem* for this lab\n",
    "* `algoProx.ipynb` will contain the proximal *algorithms* studied in this lab\n",
    "\n",
    "---\n",
    "\n",
    "The following script will allow you to import *notebooks* as if you imported *python files* and will have to be executed at each time you launch Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import start\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Composite minimization for machine learning.\n",
    "\n",
    "In this lab, we will investigate optimization algorithms over composite functions composed of a smooth and a non-smooth part using the proximal gradient algorithm over a practical problem of machine learning: binary classification using logistic regression.</br>\n",
    "\n",
    "> Read the file `logistic_regression_2.ipynb` containing the problem explanation and simulators. \n",
    "\n",
    "> Implement the proximal operation linked to $\\ell_1$ norm in the regularization. \n",
    "\n",
    "> Implement the proximal gradient algorithm in the file `algoProx.ipynb` and test you algorithm below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algoProx             # load our algoProx module (from notebook)\n",
    "reload(algoProx)            # reload the module if changed (and saved)\n",
    "from algoProx import *      # import all methods of the module into the current environment\n",
    "\n",
    "import numpy as np\n",
    "import logistic_regression_2 as pb\n",
    "reload(pb)\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 1e-5                     # Sought precision\n",
    "ITE_MAX = 1000                      # Max number of iterations\n",
    "x0      = np.zeros(pb.n)              # Initial point\n",
    "step    = 1.0/pb.L\n",
    "\n",
    "##### gradient algorithm\n",
    "x,x_tab = proximal_gradient_algorithm(pb.F , pb.f_grad , pb.g_prox , x0 , step , PREC, ITE_MAX , True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Investigate the decrease of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "F = []\n",
    "for i in range(x_tab.shape[0]):\n",
    "    F.append(pb.F(x_tab[i])) \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(F, color=\"black\", linewidth=1.0, linestyle=\"-\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot, with the following command, the support of the vector $x_k$ (i.e. one point for every non-null coordinate of $x_k$) versus the iterations. \n",
    "\n",
    "> What do yo notice? Was it expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for i in np.arange(0,x_tab.shape[0],int(x_tab.shape[0]/40)):\n",
    "    for j in range(pb.n):\n",
    "        if np.abs(x_tab[i,j])>1e-14:\n",
    "            plt.plot( i , j  , 'ko')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.ylabel('Non-null Coordinates')\n",
    "plt.xlabel('Nb. Iterations')\n",
    "plt.ylim(-1,pb.d+1)\n",
    "plt.yticks(np.arange(0,pb.d+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Regularization path.\n",
    "\n",
    "\n",
    "We saw above that the algorithm *selected* some coordinates as the other get to zero. Considering our machine learning task (see `logistic_regression_2.ipynb`), this translates into the algorithm selecting a subset of the features that will be used for the prediction step.  \n",
    "\n",
    "> Change the parameter $\\lambda_1$ of the problem (`pb.lam1`) in the code above and investigate how it influences the number of selected features.\n",
    "\n",
    "In order to quantify the influence of this feature selection, let us consider the *regularization path* that is the support of the final points obtained by our minimization method versus the value of $\\lambda_1$.\n",
    "\n",
    "> For $\\lambda_1 = 2^{-12},2^{-11}, .. , 2^{1}$, run the proximal gradient algorithm on the obtained problem and store the support of the final point, the prediction performance on the *training set* (`pb.prediction_train`) and on the *testing set* (`pb.prediction_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import algoProx             # load our algoProx module (from notebook)\n",
    "reload(algoProx)            # reload the module if changed (and saved)\n",
    "from algoProx import *      # import all methods of the module into the current environment\n",
    "\n",
    "import numpy as np\n",
    "import logistic_regression_2 as pb\n",
    "reload(pb)\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 1e-5                     # Sought precision\n",
    "ITE_MAX = 500                      # Max number of iterations\n",
    "x0      = np.zeros(pb.n)              # Initial point\n",
    "step    = 1.0/pb.L\n",
    "\n",
    "# FILL THERE #######\n",
    "reg_l1_tab = ...\n",
    "pb.lam2 = 1e-1\n",
    "\n",
    "train_perf = np.zeros(len(reg_l1_tab))\n",
    "test_perf = np.zeros_like(train_perf)\n",
    "x_tab = np.zeros((len(reg_l1_tab), pb.n))\n",
    "\n",
    "for i, lam1 in enumerate(reg_l1_tab):\n",
    "    pb.lam1 = lam1\n",
    "    # FILL HERE #########\n",
    "    # ###################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot the *regularization path* and look at the feature signification (file `student.txt` or `logistic_regression_2.ipynb`) to see which are the most important features of the dataset.\n",
    "\n",
    "> (Bonus: you can do some text manipulation to put the labels on the plot as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i, feat in enumerate(x_tab.T):\n",
    "    nonzeros = np.flatnonzero(feat)\n",
    "    plt.scatter(nonzeros - 12, (i+1)*np.ones_like(nonzeros), color='k', marker='o')\n",
    "\n",
    "plt.ylabel('Non-null Coordinates')\n",
    "\n",
    "plt.ylim(-1,pb.d+1)\n",
    "plt.yticks(np.arange(0,pb.d+1))\n",
    "plt.xlabel(\"log(lam1)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot the *training* and *testing* accuracies versus the value of $\\lambda_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lam = np.arange(-11, 2)\n",
    "# FILL HERE ####\n",
    "# ##############\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import algoProx             # load our algoProx module (from notebook)\n",
    "reload(algoProx)            # reload the module if changed (and saved)\n",
    "from algoProx import *      # import all methods of the module into the current environment\n",
    "\n",
    "import numpy as np\n",
    "import logistic_regression_2 as pb\n",
    "reload(pb)\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 1e-3                     # Sought precision\n",
    "ITE_MAX = 5000                     # Max number of iterations\n",
    "x0      = np.zeros(pb.n)              # Initial point\n",
    "step    = 1.0/pb.L\n",
    "\n",
    "log_lam = np.arange(-7, 3)\n",
    "reg_l2_tab = np.power( 2.0, log_lam )\n",
    "pb.lam1 = 1e-5\n",
    "\n",
    "train_perf = np.zeros(len(reg_l2_tab))\n",
    "test_perf = np.zeros_like(train_perf)\n",
    "x_tab = np.zeros((len(reg_l2_tab), pb.n))\n",
    "x_tab_lst = []\n",
    "\n",
    "for i, lam2 in enumerate(reg_l2_tab):\n",
    "    pb.lam2 = lam2\n",
    "    x, x_tab_alg = proximal_gradient_algorithm(pb.F , pb.f_grad , pb.g_prox , x0 , step , PREC, ITE_MAX, True)\n",
    "    x_tab[i] = x\n",
    "    x_tab_lst.append(x_tab_alg)\n",
    "    _, train_perf[i] = pb.prediction_train(x, False)\n",
    "    _, test_perf[i] = pb.prediction_test(x, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Explore the proximal algorithm or propose ideas (cite your sources if you use pieces of litterature) to change it or compare it to something else. Send the results to your favorite TA by zipping/tarballing/... your work and either:\n",
    "> * sending it directly via an email.\n",
    "> * sending and email with an invitation to a PRIVATE repository (github, gitlab, bitbucket, etc) containing your work.\n",
    ">\n",
    "> ### Guidelines:\n",
    "> Write your own code, do not try to throw LLM nonsense to your TA.\n",
    "> \n",
    "> Be original.\n",
    "> \n",
    "> Write every idea you have in the notebook, as verbosely as possible.\n",
    ">\n",
    "> Write clean code. E.g. go see python's pep8."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
