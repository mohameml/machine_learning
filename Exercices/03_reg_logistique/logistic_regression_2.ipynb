{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Problem\n",
    "\n",
    "[The work you'l have to do is about $\\|\\cdot\\|_1$; see todo at the middle of the document]\n",
    "\n",
    "### Machine Learning as an Optimization problem\n",
    "\n",
    "We have some *data*  $\\mathcal{D}$ consisting of $m$ *examples* $\\{d_i\\}$; each example consisting of a *feature* vector $a_i\\in\\mathbb{R}^d$ and an *observation* $b_i\\in \\mathcal{O}$: $\\mathcal{D} = \\{[a_i,b_i]\\}_{i=1..m}$. In this lab, we will consider the <a href=\"http://archive.ics.uci.edu/ml/datasets/Student+Performance\">student performance</a> dataset.\n",
    "\n",
    "\n",
    "The goal of *supervised learning* is to construct a predictor for the observations when given feature vectors.\n",
    "\n",
    "\n",
    "A popular approach is based on *linear models* which are based on finding a *parameter* $x$ such that the real number $\\langle a_i , x \\rangle$ is used to predict the value of the observation through a *predictor function* $g:\\mathbb{R}\\to \\mathcal{O}$: $g(\\langle a_i , x \\rangle)$ is the predicted value from $a_i$.\n",
    "\n",
    "\n",
    "In order to find such a parameter, we use the available data and a *loss* $\\ell$ that penalizes the error made between the predicted $g(\\langle a_i , x \\rangle)$ and observed $b_i$ values. For each example $i$, the corresponding error function for a parameter $x$ is $f_i(x) =   \\ell( g(\\langle a_i , x \\rangle) ; b_i )$. Using the whole data, the parameter that minimizes the total error is the solution of the minimization problem\n",
    "$$ \\min_{x\\in\\mathbb{R}^d}  \\frac{1}{m} \\sum_{i=1}^m f_i(x) = \\frac{1}{m} \\sum_{i=1}^m  \\ell( g(\\langle a_i , x \\rangle) ; b_i ). $$\n",
    "\n",
    "\n",
    "### Binary Classification with Logisitic Regression\n",
    "\n",
    "In our setup, the observations are binary: $\\mathcal{O} = \\{-1 , +1 \\}$, and the *Logistic loss* is used to form the following optimization problem\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) + \\frac{\\lambda}{2} \\|x\\|_2^2.\n",
    "\\end{align*}\n",
    "where the last term is added as a regularization (of type $\\ell_2$, aka Tikhnov) to prevent overfitting.\n",
    "\n",
    "Under some statistical hypotheses, $x^\\star = \\arg\\min f(x)$ maximizes the likelihood of the labels knowing the features vector. Then, for a new point $d$ with features vector $a$, \n",
    "$$ p_1(a) = \\mathbb{P}[d\\in \\text{ class }  +1] = \\frac{1}{1+\\exp(-\\langle a;x^\\star \\rangle)} $$\n",
    "Thus, from $a$, if $p_1(a)$ is close to $1$, one can decide that $d$ belongs to class $1$; and the opposite decision if $p(a)$ is close to $0$. Between the two, the appreciation is left to the data scientist depending on the application.\n",
    "\n",
    "\n",
    "# Regularized Problem\n",
    "\n",
    "In class, we considered $\\ell_2$ (aka Tikhnov) regularization to prevent overfitting. The whole function was smooth and thus gradient algorithms were efficient. In this lab, we will consider an $\\ell_1$ regularization to promote sparsity of the iterates. The new function (below) is non-smooth but it has a smooth part, $f$, the same as in Lab3; and a non-smooth part, $g$, that we will treat with proximal operations.\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } F(x) := \\underbrace{ \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) + \\frac{\\lambda_2}{2} \\|x\\|_2^2}_{f(x)} + \\underbrace{\\lambda_1 \\|x\\|_1 }_{g(x)}.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "# Features signification\n",
    "\n",
    "The dataset is comprised of $27$ features described below and the goal is to predict if the student may pass its year or not. It is thus of importance to investigate which features are the most significant for the student success. We will see how the $\\ell_1$ regularization can help to this goal."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1 sex - student's sex (binary: \"F\" - female or \"M\" - male)\n",
    "2 age - student's age (numeric: from 15 to 22)\n",
    "3 address - student's home address type (binary: \"U\" - urban or \"R\" - rural)\n",
    "4 famsize - family size (binary: \"LE3\" - less or equal to 3 or \"GT3\" - greater than 3)\n",
    "5 Pstatus - parent's cohabitation status (binary: \"T\" - living together or \"A\" - apart)\n",
    "6 Medu - mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\n",
    "7 Fedu - father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\n",
    "8 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n",
    "9 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n",
    "10 failures - number of past class failures (numeric: n if 1<=n<3, else 4)\n",
    "11 schoolsup - extra educational support (binary: yes or no)\n",
    "12 famsup - family educational support (binary: yes or no)\n",
    "13 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n",
    "14 activities - extra-curricular activities (binary: yes or no)\n",
    "15 nursery - attended nursery school (binary: yes or no)\n",
    "16 higher - wants to take higher education (binary: yes or no)\n",
    "17 internet - Internet access at home (binary: yes or no)\n",
    "18 romantic - with a romantic relationship (binary: yes or no)\n",
    "19 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n",
    "20 freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n",
    "21 goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n",
    "22 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "23 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "24 health - current health status (numeric: from 1 - very bad to 5 - very good)\n",
    "25 absences - number of school absences (numeric: from 0 to 93)\n",
    "26 G1 - first period grade (numeric: from 0 to 20)\n",
    "27 G2 - second period grade (numeric: from 0 to 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#### File reading\n",
    "dat_file = np.load('student.npz')\n",
    "A = dat_file['A_learn']\n",
    "final_grades = dat_file['b_learn']\n",
    "m = final_grades.size\n",
    "b = np.zeros(m)\n",
    "for i in range(m):\n",
    "    if final_grades[i]>11:\n",
    "        b[i] = 1.0\n",
    "    else:\n",
    "        b[i] = -1.0\n",
    "\n",
    "A_test = dat_file['A_test']\n",
    "final_grades_test = dat_file['b_test']\n",
    "m_test = final_grades_test.size\n",
    "b_test = np.zeros(m_test)\n",
    "for i in range(m_test):\n",
    "    if final_grades_test[i]>11:\n",
    "        b_test[i] = 1.0\n",
    "    else:\n",
    "        b_test[i] = -1.0\n",
    "\n",
    "\n",
    "d = 27 # features\n",
    "n = d+1 # with the intercept\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lam2 = 0.1 # for the 2-norm regularization best:0.1\n",
    "lam1 = 0.03 # for the 1-norm regularization best:0.03\n",
    "\n",
    "\n",
    "L = 0.25*max(np.linalg.norm(A,2,axis=1))**2 + lam2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracles\n",
    "\n",
    "### Related to function $f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    l = 0.0\n",
    "    for i in range(A.shape[0]):\n",
    "        if b[i] > 0 :\n",
    "            l += np.log( 1 + np.exp(-np.dot( A[i] , x ) ) ) \n",
    "        else:\n",
    "            l += np.log( 1 + np.exp(np.dot( A[i] , x ) ) ) \n",
    "    return l/m + lam2/2.0*np.dot(x,x)\n",
    "\n",
    "def f_grad(x):\n",
    "    g = np.zeros(n)\n",
    "    for i in range(A.shape[0]):\n",
    "        if b[i] > 0:\n",
    "            g += -A[i]/( 1 + np.exp(np.dot( A[i] , x ) ) ) \n",
    "        else:\n",
    "            g += A[i]/( 1 + np.exp(-np.dot( A[i] , x ) ) ) \n",
    "    return g/m + lam2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bonus: make this code faster by using numpy's arrays to parallelize (i.e. vectorize) the for loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related to function $g$ [TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def g_prox(y,gamma):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related to function $F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def F(x):\n",
    "    return f(x) + g(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_train(w,PRINT):\n",
    "    pred = np.zeros(A.shape[0])\n",
    "    perf = 0\n",
    "    for i in range(A.shape[0]):\n",
    "        p = 1.0/( 1 + np.exp(-np.dot( A[i] , w ) ) )\n",
    "        if p>0.5:\n",
    "            pred[i] = 1.0\n",
    "            if b[i]>0:\n",
    "                correct = \"True\"\n",
    "                perf += 1\n",
    "            else:\n",
    "                correct = \"False\"\n",
    "            if PRINT:\n",
    "                print(\"True class: {:d} \\t-- Predicted: {} \\t(confidence: {:.1f}%)\\t{}\".format(int(b[i]),1,(p-0.5)*200,correct))\n",
    "        else:\n",
    "            pred[i] = -1.0\n",
    "            if b[i]<0:\n",
    "                correct = \"True\"\n",
    "                perf += 1\n",
    "            else:\n",
    "                correct = \"False\"\n",
    "            if PRINT:\n",
    "                print(\"True class: {:d} \\t-- Predicted: {} \\t(confidence: {:.1f}%)\\t{}\".format(int(b[i]),-1,100-(0.5-p)*200,correct))\n",
    "    return pred,float(perf)/A.shape[0]\n",
    "\n",
    "def prediction_test(w,PRINT):\n",
    "    pred = np.zeros(A_test.shape[0])\n",
    "    perf = 0\n",
    "    for i in range(A_test.shape[0]):\n",
    "        p = 1.0/( 1 + np.exp(-np.dot( A_test[i] , w ) ) )\n",
    "        if p>0.5:\n",
    "            pred[i] = 1.0\n",
    "            if b_test[i]>0:\n",
    "                correct = \"True\"\n",
    "                perf += 1\n",
    "            else:\n",
    "                correct = \"False\"\n",
    "            if PRINT:\n",
    "                print(\"True class: {:d} \\t-- Predicted: {} \\t(confidence: {:.1f}%)\\t{}\".format(int(b[i]),1,(p-0.5)*200,correct))\n",
    "        else:\n",
    "            pred[i] = -1.0\n",
    "            if b_test[i]<0:\n",
    "                correct = \"True\"\n",
    "                perf += 1\n",
    "            else:\n",
    "                correct = \"False\"\n",
    "            if PRINT:\n",
    "                print(\"True class: {:d} \\t-- Predicted: {} \\t(confidence: {:.1f}%)\\t{}\".format(int(b[i]),-1,100-(0.5-p)*200,correct))\n",
    "    return pred,float(perf)/A_test.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bonus: make this code faster by using numpy's arrays to parallelize (i.e. vectorize) the for loops. (You can forget the printing.)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
