# cour 02 : **Apprentissage Supervis√© :**  


## 1. **Introduction:**


L'apprentissage supervis√© est l'une des branches fondamentales du Machine Learning, o√π les mod√®les sont entra√Æn√©s √† partir d'un ensemble de donn√©es √©tiquet√©es.  l'apprentissage supervis√© repose sur des exemples o√π chaque entr√©e est associ√©e √† une sortie d√©sir√©e.

>**Apprentissage Supervis√© :** Les mod√®les sont entra√Æn√©s sur un ensemble de donn√©es √©tiquet√©es, o√π l'algorithme apprend √† faire des pr√©dictions en se basant sur des exemples de paires entr√©e-sortie.


## 2. **Les 4 notions clefs du Apprentissage Supervis√©:**

- Pour ma√Ætriser l‚Äôapprentissage supervis√©, il faut absolument comprendre et connaitre les 4 notions suivantes :
    - **Le Dataset**
    - **Le Mod√®le et ses param√®tres**
    - **La Fonction Co√ªt**
    - **L‚ÄôAlgorithme d‚Äôapprentissage**

### 2.1 **Le Dataset:**

- **Description :** Le dataset est l'ensemble de donn√©es utilis√© pour entra√Æner, valider et tester le mod√®le. Il se compose de caract√©ristiques (``features``) qui d√©crivent les exemples et de labels ( ``traget``) qui repr√©sentent les sorties attendues pour chaque exemple. 

En Machine Learning, on compile ces exemples (ùíô, ùíö) dans un tableau  que l‚Äôon appelle **Dataset** : 

- La variable ùíö porte le nom de *target* (la cible). C‚Äôest la  valeur que l‚Äôon cherche √† pr√©dire. 

- La variable ùíô porte le nom de *feature* (facteur). Un facteur  influence la valeur de ùíö, et on a en g√©n√©ral beaucoup de  features (ùíôùüè, ùíôùüê, ‚Ä¶ ) dans notre Dataset que l‚Äôon regroupe dans une matrice ùëø.  


<img src="images/Dataset.png" width="500">


- **formalement:**

Formellement, un ensemble de donn√©es peut √™tre repr√©sent√© par une matrice de caract√©ristiques $X$ et un vecteur cible $Y$ de la mani√®re suivante :


#### Matrice de Caract√©ristiques $X$  :
- $X$ est une matrice o√π chaque ligne repr√©sente un exemple d'entra√Ænement, et chaque colonne repr√©sente une caract√©ristique de cet exemple.
- Si vous avez $m$ exemples d'entra√Ænement et $n$ caract√©ristiques, alors $X$ est une matrice de dimensions $m \times n$.
- La notation $X_{ij}$ repr√©sente la valeur de la caract√©ristique $j$ pour l'exemple $i$.

Formellement, $X$ peut √™tre d√©fini comme suit :

$$
X = \begin{bmatrix} 
X_{11} & X_{12} & \ldots & X_{1n} \\
X_{21} & X_{22} & \ldots & X_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
X_{m1} & X_{m2} & \ldots & X_{mn}
\end{bmatrix} 
$$

#### Vecteur Cible ($Y$) :
- $Y$ est un vecteur qui contient les sorties attendues (√©tiquettes) pour chaque exemple d'entra√Ænement.
- Si vous avez $m$ exemples d'entra√Ænement, alors $Y$ est un vecteur de dimensions $m \times 1$.
- La notation $Y_i$ repr√©sente la valeur de la cible pour l'exemple $i$.

Formellement, $Y$ peut √™tre d√©fini comme suit :

$$ 
Y = \begin{bmatrix} 
Y_{1} \\
Y_{2} \\
\vdots \\
Y_{m}
\end{bmatrix} 
$$




### 2.2 **Le Mod√®le et ses Param√®tres :**

- **Description :** Le mod√®le est l'algorithme ou la structure math√©matique utilis√© pour faire des pr√©dictions √† partir des donn√©es d'entr√©e. Les mod√®les ont des param√®tres ajustables qui sont appris pendant l'entra√Ænement. Ces param√®tres sont ajust√©s pour minimiser une fonction de co√ªt.


Voici quelques-uns des mod√®les couramment utilis√©s en apprentissage supervis√© :

- **R√©gression Lin√©aire :**
   - **Type de Probl√®me :** R√©gression (pr√©diction de valeurs continues).
   - **Description :** Mod√®le qui tente de trouver une relation lin√©aire entre les caract√©ristiques et la cible.

- **R√©gression Logistique :**
   - **Type de Probl√®me :** Classification binaire.
   - **Description :** Utilis√© pour mod√©liser la probabilit√© d'appartenance √† une classe.

- **Machines √† Vecteurs de Support (SVM) :**
   - **Type de Probl√®me :** Classification ou r√©gression.
   - **Description :** Trouve l'hyperplan qui maximise la marge entre les diff√©rentes classes.

- **Arbres de D√©cision :**
   - **Type de Probl√®me :** Classification ou r√©gression.
   - **Description :** Structure arborescente o√π chaque n≈ìud repr√©sente une d√©cision bas√©e sur une caract√©ristique.

![image](images/modele.jpeg)


### 2.3. **La Fonction Co√ªt :**

- **Description :** La fonction co√ªt mesure la diff√©rence entre les pr√©dictions du mod√®le et les sorties r√©elles (√©tiquettes) dans l'ensemble d'entra√Ænement. 

L'objectif de l'apprentissage est de minimiser cette fonction co√ªt. Des fonctions co√ªt courantes incluent la somme des carr√©s des erreurs pour les probl√®mes de r√©gression et l'entropie crois√©e pour les probl√®mes de classification.

![images](images/f_cout.jpeg)

- **Formellement:**

La fonction de co√ªt (ou fonction d'erreur) mesure la diff√©rence entre les pr√©dictions d'un mod√®le et les valeurs r√©elles dans l'ensemble d'entra√Ænement. 

L'objectif est de minimiser cette fonction de co√ªt pendant le processus d'apprentissage. 

Formellement, la fonction de co√ªt ($J$) peut √™tre d√©finie comme suit, en fonction du type de probl√®me (r√©gression, classification, etc.) :

De mani√®re g√©n√©rale, la fonction de co√ªt ($J$) en apprentissage supervis√© mesure l'√©cart entre les pr√©dictions du mod√®le ($f_\theta(x^{(i)})$) et les valeurs r√©elles ($y^{(i)}$) pour chaque exemple d'entra√Ænement. 

La forme pr√©cise de la fonction de co√ªt d√©pend du type de probl√®me que vous r√©solvez (r√©gression, classification, etc.).

La formulation g√©n√©rale pour la fonction de co√ªt peut √™tre exprim√©e comme suit :

$$ J(\theta) = \frac{1}{m} \sum_{i=1}^{m} L(f_\theta(x^{(i)}), y^{(i)}) $$

o√π :
- $m$ est le nombre d'exemples d'entra√Ænement.
- $f_\theta(x^{(i)})$ est la pr√©diction du mod√®le pour l'exemple $i$.
- $y^{(i)}$ est la valeur r√©elle (√©tiquette) de l'exemple $i$.
- $L$ est une fonction de perte (loss function) qui mesure l'√©cart entre la pr√©diction et la vraie valeur.

La fonction de perte ($L$) d√©pend du type de probl√®me. 

La cl√© est d'ajuster les param√®tres ($\theta$) du mod√®le pour minimiser cette fonction de co√ªt, ce qui se fait g√©n√©ralement √† l'aide d'algorithmes d'optimisation tels que la descente de gradient.


- **Exemples :**

- **Probl√®me de R√©gression :**
    - **Fonction de Co√ªt (R√©gression Lin√©aire) :**

$$ 
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2 
$$

o√π $m$ est le nombre d'exemples d'entra√Ænement, $h_\theta(x^{(i)})$ est la pr√©diction du mod√®le pour l'exemple $i$, $y^{(i)}$ est la valeur r√©elle, et $\theta$ sont les param√®tres du mod√®le.

    
- **Probl√®me de Classification Binaire :**
    - **Fonction de Co√ªt (R√©gression Logistique) :**
        
$$ 
J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)}))] 
$$


o√π $m$ est le nombre d'exemples d'entra√Ænement, $h_\theta(x^{(i)})$ est la probabilit√© pr√©dite par le mod√®le que l'exemple $i$ appartienne √† la classe positive, $y^{(i)}$ est la valeur r√©elle (0 ou 1), et $\theta$ sont les param√®tres du mod√®le.

- **Probl√®me de Classification Multiclasse (Softmax) :**
    - **Fonction de Co√ªt (Softmax) :**
        
$$ 
J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{K} [y_k^{(i)} \log(h_\theta(x^{(i)})_k)] 
$$

o√π $m$ est le nombre d'exemples d'entra√Ænement, $K$ est le nombre de classes, $h_\theta(x^{(i)})_k$ est la probabilit√© pr√©dite par le mod√®le que l'exemple $i$ appartienne √† la classe $k$, $y_k^{(i)}$ est 1 si l'exemple $i$ appartient √† la classe $k$ et 0 sinon, et $\theta$ sont les param√®tres du mod√®le.




### 2.4. **L'Algorithme d'Apprentissage :**

- **Description :** L'algorithme d'apprentissage est la m√©thode utilis√©e pour ajuster les param√®tres du mod√®le afin de minimiser la fonction co√ªt. Il s'agit souvent d'algorithmes d'optimisation tels que la descente de gradient stochastique (SGD) qui ajustent progressivement les param√®tres pour converger vers une configuration qui produit des pr√©dictions pr√©cises.

- **Exemples d'algorithmes d'apprentissage couramment utilis√©s :**

    - **Descente de Gradient (Gradient Descent) :**
        - **Type :** Algorithme d'optimisation.
        - **Description :** Ajuste progressivement les param√®tres du mod√®le dans la direction oppos√©e du gradient de la fonction de co√ªt pour minimiser cette fonction.

    - **Stochastic Gradient Descent (SGD) :**
        - **Type :** Variation de la descente de gradient.
        - **Description :** Une approche plus efficace o√π les mises √† jour des param√®tres sont effectu√©es pour un seul exemple d'entra√Ænement √† la fois.

    - **Mini-Batch Gradient Descent :**
        - **Type :** Variation de la descente de gradient.
        - **Description :** Les mises √† jour des param√®tres sont effectu√©es sur un petit sous-ensemble (mini-batch) des exemples d'entra√Ænement.

    - **Random Forest :**
        - **Type :** M√©thode d'ensemble.
        - **Description :** Utilise un ensemble d'arbres de d√©cision pour am√©liorer la robustesse et la g√©n√©ralisation du mod√®le.


    - **K-Nearest Neighbors (KNN) :**
        - **Type :** Algorithme bas√© sur l'instance.
        - **Description :** Classifie un point en se basant sur les classes des k voisins les plus proches.


