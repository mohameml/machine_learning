## ğŸ¤– **RÃ©seaux de Neurones (Neural Networks)**

Les **rÃ©seaux de neurones artificiels (ANNs - Artificial Neural Networks)** sont des **modÃ¨les d'apprentissage automatique inspirÃ©s du cerveau humain**. Ils sont utilisÃ©s dans plusieurs applications comme la classification, la rÃ©gression, la reconnaissance d'images et le traitement du langage naturel.

---

## ğŸ”¹ **Structure d'un RÃ©seau de Neurones**

Un rÃ©seau de neurones est composÃ© de plusieurs couches de **neurones artificiels** :

1ï¸âƒ£ **Couche d'entrÃ©e (Input Layer)** : ReÃ§oit les donnÃ©es d'entrÃ©e.  
2ï¸âƒ£ **Couches cachÃ©es (Hidden Layers)** : Effectuent des calculs et extraient des caractÃ©ristiques.  
3ï¸âƒ£ **Couche de sortie (Output Layer)** : Fournit le rÃ©sultat final (classe ou valeur prÃ©dite).

ğŸ“Œ **Chaque neurone applique une fonction d'activation pour introduire de la non-linÃ©aritÃ©**.

---

## ğŸ”¥ **Fonctionnement d'un Neurone**

Un neurone artificiel prend un vecteur dâ€™entrÃ©e \( X = (x_1, x_2, ..., x_n) \) et effectue un calcul sous forme de somme pondÃ©rÃ©e :

\[
z = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b
\]

Puis, il applique une **fonction d'activation** \( f(z) \) pour produire une sortie :

\[
y = f(z)
\]

ğŸ”¹ **Exemples de fonctions d'activation :**  
âœ… **SigmoÃ¯de** : \( f(z) = \frac{1}{1 + e^{-z}} \) (utilisÃ©e en classification binaire).  
âœ… **ReLU (Rectified Linear Unit)** : \( f(z) = \max(0, z) \) (rÃ©seau profond).  
âœ… **Softmax** : Pour la classification multi-classes.

---

## ğŸ“œ **Exemple en Python : RÃ©seau de Neurones avec Keras (Classification)**

```python
import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# GÃ©nÃ©ration d'un dataset en forme de lune
X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# CrÃ©ation du modÃ¨le
model = keras.Sequential([
    keras.layers.Dense(10, activation="relu", input_shape=(2,)),  # Couche cachÃ©e avec 10 neurones
    keras.layers.Dense(10, activation="relu"),  # DeuxiÃ¨me couche cachÃ©e
    keras.layers.Dense(1, activation="sigmoid")  # Couche de sortie (classification binaire)
])

# Compilation du modÃ¨le
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

# EntraÃ®nement
history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=0)

# Ã‰valuation
loss, acc = model.evaluate(X_test, y_test)
print(f"PrÃ©cision du modÃ¨le : {acc:.2f}")

# Visualisation de la frontiÃ¨re de dÃ©cision
def plot_decision_boundary(model, X, y):
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = (Z > 0.5).astype(int).reshape(xx.shape)

    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.coolwarm)
    plt.title("RÃ©seau de Neurones - FrontiÃ¨re de DÃ©cision")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.show()

# Affichage
plot_decision_boundary(model, X, y)
```

---

## ğŸ“Š **Explication du Code**

âœ… **`make_moons()`** : GÃ©nÃ¨re des donnÃ©es en forme de **lunes** pour classification.  
âœ… **`keras.Sequential()`** : DÃ©finit un rÃ©seau de neurones avec 2 couches cachÃ©es de 10 neurones.  
âœ… **`Dense(activation="relu")`** : Utilise **ReLU** pour mieux capturer les non-linÃ©aritÃ©s.  
âœ… **`Dense(activation="sigmoid")`** : Utilise **SigmoÃ¯de** pour la classification binaire.  
âœ… **Visualisation de la frontiÃ¨re de dÃ©cision** ğŸ¨.

---

## ğŸš€ **Avantages des RÃ©seaux de Neurones**

âœ… **CapacitÃ© Ã  modÃ©liser des relations complexes** (non-linÃ©aritÃ©).  
âœ… **ScalabilitÃ©** : Fonctionne bien avec de grandes quantitÃ©s de donnÃ©es.  
âœ… **UtilisÃ© pour la vision, le NLP, et bien plus**.

ğŸ“Œ **InconvÃ©nients :**  
ğŸ”¹ Besoin de **beaucoup de donnÃ©es** pour bien gÃ©nÃ©raliser.  
ğŸ”¹ Plus **lent** que des modÃ¨les simples comme la rÃ©gression logistique.  
ğŸ”¹ Difficile Ã  **interprÃ©ter** (boÃ®te noire).

Tu veux un exemple avec un **rÃ©seau plus profond (Deep Learning) et TensorFlow** ? ğŸš€
