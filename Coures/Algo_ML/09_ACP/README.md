## ğŸŒ **Analyse en Composantes Principales (ACP - PCA en anglais)**

L'**Analyse en Composantes Principales (ACP)** est une technique de **rÃ©duction de dimension** utilisÃ©e en **machine learning et statistiques** pour :  
âœ… RÃ©duire la dimension des donnÃ©es tout en conservant un maximum d'information.  
âœ… Identifier les axes de variation les plus importants dans les donnÃ©es.  
âœ… Faciliter la **visualisation** et lâ€™**interprÃ©tation** des donnÃ©es.

---

## ğŸ”¹ **Comment fonctionne l'ACP ?**

L'ACP projette les donnÃ©es sur un nouvel espace de dimension rÃ©duite en utilisant les **valeurs propres et vecteurs propres** de la matrice de covariance.

1ï¸âƒ£ **Centrage des donnÃ©es** : Soustraction de la moyenne pour avoir une moyenne de 0.  
2ï¸âƒ£ **Calcul de la matrice de covariance** : Mesure la relation entre les variables.  
3ï¸âƒ£ **DÃ©composition spectrale (SVD ou valeurs propres)** : Trouver les **composantes principales**.  
4ï¸âƒ£ **Projection des donnÃ©es** : Transformation des donnÃ©es selon les **axes principaux**.  
5ï¸âƒ£ **SÃ©lection des composantes** : On garde celles qui expliquent le plus de variance.

ğŸš€ **But :** Trouver une nouvelle base oÃ¹ les donnÃ©es sont mieux reprÃ©sentÃ©es avec moins de dimensions.

---

## ğŸ“œ **Exemple en Python avec Visualisation (ACP sur Iris)**

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

# Chargement du dataset Iris
iris = load_iris()
X = iris.data
y = iris.target
labels = iris.target_names

# Normalisation des donnÃ©es (obligatoire pour ACP)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Application de l'ACP (rÃ©duction Ã  2 dimensions pour visualisation)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Visualisation des donnÃ©es aprÃ¨s ACP
plt.figure(figsize=(8,6))
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y, palette="Set1", legend='full')
plt.xlabel("Composante Principale 1")
plt.ylabel("Composante Principale 2")
plt.title("Projection ACP des donnÃ©es Iris")
plt.show()

# Affichage de la variance expliquÃ©e par chaque composante
print(f"Variance expliquÃ©e par la premiÃ¨re composante : {pca.explained_variance_ratio_[0]:.2f}")
print(f"Variance expliquÃ©e par la deuxiÃ¨me composante : {pca.explained_variance_ratio_[1]:.2f}")
```

---

## ğŸ“Š **Explication du Code**

âœ… **Standardisation** ğŸ“ : Lâ€™ACP est sensible aux Ã©chelles, donc on normalise avec `StandardScaler()`.  
âœ… **Application de lâ€™ACP** ğŸ§® : `PCA(n_components=2)` pour rÃ©duire Ã  **2 dimensions**.  
âœ… **Visualisation** ğŸ¨ : Un **scatter plot** montre les clusters des diffÃ©rentes classes d'Iris.  
âœ… **Variance expliquÃ©e** ğŸ“ˆ : Permet de voir combien dâ€™information chaque axe retient.

---

## ğŸš€ **Avantages de lâ€™ACP**

âœ… **RÃ©duit la dimensionnalitÃ©**, utile pour des modÃ¨les plus rapides.  
âœ… **Ã‰limine la redondance** entre les variables (corrÃ©lations fortes).  
âœ… **Facilite la visualisation** des donnÃ©es en haute dimension.

ğŸ“Œ **InconvÃ©nients** :  
ğŸ”¹ Perd un peu d'interprÃ©tabilitÃ© (les nouvelles dimensions ne sont pas directement liÃ©es aux features initiales).  
ğŸ”¹ Fonctionne mieux si les variables sont bien **corrÃ©lÃ©es**.

Tu veux voir une version **3D** de la visualisation de l'ACP ? ğŸš€
