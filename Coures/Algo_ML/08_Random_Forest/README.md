## ğŸŒ³ **Random Forest : Un Algorithme d'Ensemble Apprentissage**

L'algorithme **Random Forest** est une **mÃ©thode d'ensemble** basÃ©e sur **plusieurs arbres de dÃ©cision**. Il est utilisÃ© pour la **classification** et la **rÃ©gression**.

---

## ğŸ”¹ **Comment fonctionne Random Forest ?**

1ï¸âƒ£ **CrÃ©ation de plusieurs arbres de dÃ©cision** (d'oÃ¹ le terme "forÃªt").  
2ï¸âƒ£ **Chaque arbre est entraÃ®nÃ© sur un sous-ensemble alÃ©atoire des donnÃ©es** (technique du bootstrap).  
3ï¸âƒ£ **Chaque arbre vote** pour une classe (classification) ou fait une moyenne des prÃ©dictions (rÃ©gression).  
4ï¸âƒ£ **PrÃ©diction finale :**

-   ğŸ† **Classification** : La classe majoritaire parmi les arbres.
-   ğŸ“ˆ **RÃ©gression** : Moyenne des prÃ©dictions des arbres.

ğŸ”¹ Random Forest rÃ©duit **le surapprentissage** par rapport Ã  un **arbre unique** ! ğŸ¯

---

## ğŸ“œ **Exemple en Python : Classification avec Random Forest**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# GÃ©nÃ©ration d'un dataset de classification
X, y = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0, random_state=42)

# SÃ©paration en train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ModÃ¨le Random Forest avec 100 arbres
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# PrÃ©diction et Ã©valuation
y_pred = rf.predict(X_test)
print(f"PrÃ©cision du modÃ¨le : {accuracy_score(y_test, y_pred):.2f}")

# Visualisation de la frontiÃ¨re de dÃ©cision
def plot_decision_boundary(model, X, y):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.coolwarm)
    plt.title("Random Forest - FrontiÃ¨re de DÃ©cision")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.show()

# Affichage de la frontiÃ¨re de dÃ©cision
plot_decision_boundary(rf, X, y)
```

---

## ğŸ“Š **Explication du Code**

âœ… **`make_classification()`** : GÃ©nÃ¨re un dataset avec **2 features** (pour visualisation).  
âœ… **`RandomForestClassifier(n_estimators=100)`** : Utilise **100 arbres** ğŸŒ³.  
âœ… **`fit(X_train, y_train)`** : EntraÃ®ne la forÃªt sur les donnÃ©es.  
âœ… **`accuracy_score(y_test, y_pred)`** : Ã‰value la **prÃ©cision** du modÃ¨le.  
âœ… **Visualisation de la frontiÃ¨re de dÃ©cision** ğŸ“ˆ.

---

## ğŸš€ **Avantages de Random Forest**

âœ… **RÃ©duit le surapprentissage** comparÃ© Ã  un arbre unique.  
âœ… **TrÃ¨s performant** pour les problÃ¨mes de classification et rÃ©gression.  
âœ… **GÃ¨re bien les donnÃ©es bruitÃ©es et les valeurs manquantes**.  
âœ… **Peut gÃ©rer des datasets avec de nombreuses features**.

ğŸ“Œ **InconvÃ©nients :** Plus lent que des modÃ¨les simples comme la rÃ©gression logistique.

Tu veux voir un **exemple de Random Forest pour la rÃ©gression** ? ğŸ”¥
